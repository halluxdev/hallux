backends:
    - cache: # short-name, used in command-line
        type: dummy # type : dummy / openai / hallux
        filename: dummy.json

    # - rest:
    #     type: rest
    #     url: http://localhost:8000/generate
    #     request_body: { message: $PROMPT } # Defaults to: $PROMPT
    #     response_body: $RESPONSE.answer # Defaults to $RESPONSE
    #     headers:
    #        Content-Type: application/json
    #        Authorization: Bearer <KEY>
    #        API-KEY: <KEY>


    # local Ollama
    - rest:
        type: rest
        url: http://localhost:11434/api/generate
        request_body: { model: "llama3.2", prompt: $PROMPT, stream: False } # Defaults to: $PROMPT
        response_body: $RESPONSE.response # Defaults to $RESPONSE


    - gpt3:
        type: litellm
        model: gpt-3.5-turbo

    - llama3:
        model: ollama/llama3

tools:
    #ruff:
    #    # command-line arguments for ruff
    #    args:
    #mypy:
    #    # command-line arguments for mypy
    #    args: --ignore-missing-imports
    sonar:
        validity_test: ./run-validity-tests.sh -x

        # SonarQube
        url: https://sonarqube.hallux.dev
        project: halluxdev_hallux_AYpIk3Z__hwOMJbIE7XQ
        search_params: resolved=false&severities=MINOR,MAJOR,CRITICAL&statuses=OPEN,CONFIRMED,REOPENED&cleanCodeAttributeCategories=ADAPTABLE,CONSISTENT,RESPONSIBLE

        # SonarCloud
        # url: https://sonarcloud.io
        # project: halluxdev_hallux
        # search_params: projects=halluxdev_hallux&resolved=false&severities=MINOR,MAJOR,CRITICAL&statuses=OPEN,CONFIRMED,REOPENED

groups:
    all: ["ruff", "mypy", "sonar", "cpp"]
    python: ["ruff", "mypy"]

