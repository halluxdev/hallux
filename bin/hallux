#!/bin/env python
# Copyright: Hallux team, 2023

# MAIN COMMAND-LINE EXECUTABLE
# - Runs checks in the current folder (linting / unit-tests / docstrings / compilation errors)
# - Extracts every single message
# - Makes a prompt for every message
# - Sends prompt to GPT, receives answer,
# - Changes code right in the codebase or sends this change as Github Web GUI proposal


from __future__ import annotations
import subprocess
import os
import sys
# from abc import ABC, abstractmethod
from typing import Final
import yaml
from pathlib import Path
import openai
from openai.api_resources import ChatCompletion
from diff_target import DiffTarget, FilesystemTarget, GitCommitTarget, GithubProposalTraget


def load_config() -> tuple[dict | None, Path | None]:
    curr_path = Path(os.getcwd())
    while not curr_path.joinpath(".hallux").exists() and curr_path.parent != curr_path:
        curr_path = curr_path.parent
    if not curr_path.joinpath(".hallux").exists():
        return None, None
    config_file = str(curr_path.joinpath(".hallux"))
    with open(config_file) as file_stream:
        yaml_dict = yaml.load(file_stream, Loader=yaml.CLoader)
    return yaml_dict, curr_path


#
#
# class PromptExecutor(ABC):
#     def __init__(self):
#         pass
#
#     @abstractmethod
#     def process(self):
#         pass
#
# class FeatureExecutor(ABC):
#     def __init__(self):
#         pass
#
#     @abstractmethod
#     def process(self):
#         pass
#
#
# class PythonRuffLinting(FeatureExecutor):
#     def __init__(self, params, diff_target: DiffTarget):
#         super().__init__()
#         self.diff_target = diff_target
#
#     def process(self):



class Hallux:
    def __init__(self, config: dict, path: Path, diff_target: DiffTarget):
        self.model: Final[str] = config["backend"]["model"] if "model" in config["backend"].keys() else None
        self.config: Final[dict] = config
        self.path: Final[Path] = path
        self.diff_target: Final[DiffTarget] = diff_target
        self.debug: bool = True
        if self.model is None:
            print("Error: Backend has no model")
            exit(3)

    def process(self):
        if "python" in self.config.keys():
            self.python()
        if "cpp" in self.config.keys():
            self.cpp()

    def python(self):
        if "linting" in self.config["python"].keys():
            self.python_linting(config["python"]["linting"])
        if "tests" in config["python"].keys():
            self.python_tests(config["python"]["tests"])
        if "docstrings" in config["python"].keys():
            self.python_docstrings(config["python"]["docstrings"])

    def python_linting(self, params: str | None):
        if params is None or params == "ruff":
            self.python_linting_ruff(["ruff", "check", "."])
        else:
            print("We only support RUFF for python lint")
            exit(5)

    def python_linting_ruff(self, command):
        print("Process python linting issues:")

        try:
            subprocess.check_output(command)
            print("No python linting issues found")
        except subprocess.CalledProcessError as e:
            ruff_output = e.output

            warnings: list[str] = str(ruff_output.decode("utf-8")).split("\n")
            for warn in warnings[:-2]:
                print(warn)
                filename = warn.split(" ")[0].split(":")[0]
                warn_line = int(warn.split(" ")[0].split(":")[1])
                with open(filename, "rt") as file:
                    filelines = file.read().split("\n")

                request = "Fix python linting issue, write resulting code only:\n"
                request = request + warn + "\n"
                request = request + "Excerpt from the corresponding python file (not full):\n"
                start_line = max(0, warn_line - 4)
                end_line = min(len(filelines) - 1, warn_line + 4)
                added_comment: str = " # line " + str(warn_line)
                for line_index in range(start_line, end_line):
                    request = request + filelines[line_index]
                    if line_index + 1 == warn_line:
                        request = request + added_comment
                    request = request + "\n"
                print("request")
                print(request)

                resulting_code : str = ''

                if self.debug:
                    resulting_code = "        try:\n" + \
                                     "            token1 = next(tokens1)\n" + \
                                     "            token2 = next(tokens2)\n" + \
                                     "        except:\n" + \
                                     "            break\n\n@pytest.mark.parametrize(\n"

                else:
                    result = ChatCompletion.create(messages=[{"role": "user", "content": request}], model=self.model)
                    print("result")
                    print(result)

                    if len(result["choices"]) > 0:
                        print(result["choices"])
                        for variant in result["choices"]:
                            resulting_code = variant["message"]["content"]


                if len(resulting_code) > 0:
                    resulting_lines = resulting_code.split("\n")
                    for i in range(len(resulting_lines)):
                        line : str = resulting_lines[i]
                        if line.endswith(added_comment):
                            resulting_lines[i] = line[:-len(added_comment)]
                            break
                    self.diff_target.apply_diff(filename, start_line, end_line, resulting_lines, warn)

                if self.debug:
                    break

    def python_tests(self, params: dict | str):
        pass

    def python_docstrings(self, params: dict | str):
        pass

    def cpp(self):
        # if "cpp" in config.keys():
        #     if "compile" in config["cpp"].keys():
        #         cpp_compile(config["cpp"]["compile"], model)
        #     if "linking" in config["cpp"].keys():
        #         cpp_linking(config["cpp"]["linking"], model)
        #     if "tests" in config["cpp"].keys():
        #         cpp_tests(config["cpp"]["tests"], model)
        pass

    def cpp_compile(self, params: dict | str):
        pass

    def cpp_linking(self, params: dict | str):
        pass

    def cpp_tests(self, params: dict | str):
        pass


if __name__ == "__main__":
    print("Hallux v0.1 - Convenient Coding Assistant")
    if len(sys.argv) < 2 or sys.argv[1] not in ["fix", "propose", "commit"]:
        print("Usage: hallux fix | hallux commit  | hallux propose Github_PullRequest_ID")
        exit(0)

    config, path = load_config()
    if config is None:
        print("Error: config file not found")
        exit(1)

    target: DiffTarget

    if sys.argv[1] == "fix":
        target = FilesystemTarget()
    elif sys.argv[1] == "commit":
        target = GitCommitTarget()
    else:
        target = GithubProposalTraget(sys.argv[2])

    os.chdir(str(path))

    openai.api_key = os.getenv("OPENAI_API_KEY")

    hallux = Hallux(config=config, path=path, diff_target=target)

    hallux.process()

