#!/usr/bin/env python
# Copyright: Hallux team, 2023

# MAIN COMMAND-LINE EXECUTABLE
# - Runs checks in the current folder (linting / unit-tests / docstrings / compilation errors)
# - Extracts every single message
# - Makes a prompt for every message
# - Sends prompt to GPT, receives answer,
# - Changes code right in the codebase or sends this change as Github Web GUI proposal

from __future__ import annotations
import os
import sys
from typing import Final
import yaml
from pathlib import Path

from query_backend import QueryBackend, OpenAiChatGPT, DummyBackend
from targets.diff_target import DiffTarget
from targets.filesystem_target import FilesystemTarget
from targets.git_commit_target import GitCommitTarget
from targets.github_proposal_traget import GithubProposalTraget
from processors.cpp.cpp import CppProcessor
from processors.python.python import PythonProcessor

DEBUG : Final[bool] = False
CONFIG_FILE : Final[str] = ".hallux"

class Hallux:
    def __init__(self, query_backend: QueryBackend, config: dict, root_path: Path, diff_target: DiffTarget, command_path: Path | None = None, verbose : bool = False):
        self.query_backend = query_backend
        self.config: Final[dict] = config
        self.root_path: Final[Path] = root_path # directory where config file was found
        self.command_path: Final[Path] = command_path if command_path is not None else root_path # from where hallux was called
        self.diff_target: Final[DiffTarget] = diff_target
        self.verbose : bool = verbose

    def process(self):
        if "python" in self.config.keys():
            python = PythonProcessor(self.query_backend, self.diff_target, self.command_path, self.config["python"], self.verbose)
            python.process()
        if "cpp" in self.config.keys():
            cpp = CppProcessor(self.query_backend, self.diff_target, self.command_path, self.config["cpp"], self.verbose)
            cpp.process()

    @staticmethod
    def find_config() -> tuple[dict, Path]:
        cwd_path : Final[Path] = Path(os.getcwd())
        config_path = cwd_path
        while not config_path.joinpath(CONFIG_FILE).exists() and config_path.parent != config_path:
            config_path = config_path.parent
        if not config_path.joinpath(CONFIG_FILE).exists():
            return {}, cwd_path
        config_file = str(config_path.joinpath(CONFIG_FILE))
        with open(config_file) as file_stream:
            yaml_dict = yaml.load(file_stream, Loader=yaml.CLoader)
        return yaml_dict, config_path

    @staticmethod
    def print_usage():
        print("Hallux v0.1 - Convenient Coding Assistant")
        print("USAGE: ")
        print("hallux [COMMAND] [TARGET] [BACKEND] [PLUGINS] [DIR]")
        print()
        print("Options for [COMMAND]:")
        print("fix         (DEFAULT) Tries fixing issues just once and exists")
        print("agent       Monitors target and tries fixing issues in the infinite loop")
        print()
        print("Options for [TARGET]:")
        print("--files     (DEFAULT) Writes fixes directly into local files.")
        print("--git       Adds separate git commit for every fix,")
        print("            must be in GIT repository to enable this")
        print("--github https://BUSINESS.github.com/YOUR_NAME/REPO_NAME/pull/ID")
        print("            Submits proposals into Github Pull-Request,")
        print("            must be in GIT repository to enable this,")
        print("            head SHA in local git and on Github must be same,")
        print("            env variable GITHUB_TOKEN must be properly set,")
        print()
        print("Options for [BACKEND]: ")
        print("--dummy [DUMMY.JSON]  ")
        print("            (DEFAULT) reads/writes all queries from/to DUMMY.JSON file")
        print("            If [DUMMY.JSON] not specified it is defaulted to '.dummy.json'")
        print("--openai [MODEL-NAME] [MAX-TOKENS]")
        print("            Uses OpenAI API for queries,")
        print("            env variable OPENAI_API_KEY must be properly set.")
        print("            If [MODEL-NAME] not specified it is defaulted to gpt-3.5-turbo")
        print("            If [MAX-TOKENS] not specified it is defaulted to 4097")
        print()
        print("Options for [PLUGINS]:")
        print("--all       (DEFAULT) try all plugins, or configured ones")
        print("--python    try fixing only python issues")
        print("--cpp       try fixing only c++ issues")
        print("--ruff      try fixing only only ruff issues")
        print("--gcc-make  try fixing only only gcc-make issues")


    @staticmethod
    def init_target(argv: list[str], config : dict) -> DiffTarget:
        i : int = 0
        while i < len(argv):
            arg = argv[i]
            if arg == "--github":
                if len(argv) > i and argv[i+1].startswith("https://"):
                    return GithubProposalTraget(argv[i+1])
                else:
                   raise SystemError("--github should be followed by proper URL like https://BUSINESS.github.com/YOUR_NAME/REPO_NAME/pull/ID")
            elif arg == "--git":
                return GitCommitTarget()
            i += 1
        if "target" in config:
            if isinstance(config["target"], dict) and "github" in config["target"]:
                return GithubProposalTraget(config["target"]["github"])
            elif config["target"] == "git":
                return GitCommitTarget()
        return FilesystemTarget()

    @staticmethod
    def init_backend(argv: list[str], config : dict) -> QueryBackend:
        i : int = 0
        model = "gpt-3.5-turbo"
        max_tokens = 4097
        openai_found : bool = False
        dummy_json = ".dummy.json"

        if "backend" in config and config["backend"] is not None:
            if "openai" in config["backend"] and config["backend"]["openai"] is not None:
                openai_found = True
                if "model" in config["backend"]["openai"]:
                    model = config["backend"]["openai"]["model"]
                if "max_tokens" in config["backend"]["openai"]:
                    max_tokens = config["backend"]["openai"]["max_tokens"]
            if "dummy" in config["backend"] and config["backend"]["dummy"] is not None:
                dummy_json = config["backend"]["dummy"]

        while i < len(argv):
            arg = argv[i]
            if arg == "--openai":
                openai_found = True
                if len(argv) >= i+1 and argv[i+1].startswith("gpt-"):
                    model = argv[i+1]

                if len(argv) >= i+2 and int(argv[i+2]) > 1000:
                    max_tokens = int(argv[i+2])
            if arg == "--dummy":
                if len(argv) >= i+1 and str(argv[i+1]).endswith(".json"):
                    dummy_json = str(argv[i+1])
            i += 1

        if openai_found:
            return OpenAiChatGPT(model, max_tokens)

        return DummyBackend(dummy_json, root_path=config_path)

    def init_plugins(argv: list[str], config : dict) -> dict:
        plugins = {"python": {"ruff" : "."},
                   "cpp": {"compile": True}}

        # plugins: dict = {"all": config["all"] if "all" in config else None,
        #                  "python": config["python"] if "python" in config else None,
        #                  "ruff": config["ruff"] if "ruff" in config else None,
        #                  "cpp": config["cpp"] if "cpp" in config else None,
        #                  "gcc-make": config["gcc-make"] if "gcc-make" in config else None,
        #                  }
        # for arg in argv:
        #     if arg.lstrip("-") in plugins.keys():
        #         plugins[arg.lstrip("-")] = True

        return plugins


if __name__ == "__main__":
    command_path : Final[Path] = Path(os.getcwd())
    config, config_path = Hallux.find_config()
    if len(sys.argv) < 2:
        Hallux.print_usage()
        exit(0)

    query_backend: QueryBackend = Hallux.init_backend(sys.argv, config)

    target: DiffTarget = Hallux.init_target(sys.argv, config)

    plugins : dict = Hallux.init_plugins(sys.argv, config)

    hallux = Hallux(query_backend=query_backend, config=plugins, root_path=config_path, diff_target=target, command_path=command_path)

    hallux.process()

